{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/sklearn/utils/deprecation.py:144: FutureWarning: The sklearn.decomposition.base module is  deprecated in version 0.22 and will be removed in version 0.24. The corresponding classes / functions should instead be imported from sklearn.decomposition. Anything that cannot be imported from sklearn.decomposition is now part of the private API.\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "#from imutils import paths\n",
    "from pathlib import Path\n",
    "import os, sys\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "\n",
    "from itertools import product\n",
    "from PIL import Image\n",
    "\n",
    "from lucent.optvis import render, param, transform, objectives\n",
    "from lucent.modelzoo.util import get_model_layers\n",
    "from lucent.misc.io import show\n",
    "from lucent.misc.channel_reducer import ChannelReducer\n",
    "from lucent.misc.io import show\n",
    "\n",
    "from captum.insights import AttributionVisualizer, Batch\n",
    "from captum.insights.features import ImageFeature\n",
    "\n",
    "# Local modules\n",
    "from cub_tools.train import train_model\n",
    "from cub_tools.visualize import imshow, visualize_model\n",
    "from cub_tools.utils import unpickle, save_pickle\n",
    "from cub_tools.transforms import makeAggresiveTransforms, makeDefaultTransforms, resizeCropTransforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def baseline_func(input):\n",
    "    return input * 0\n",
    "\n",
    "def formatted_data_iter(dataloader):\n",
    "    dataloader = iter(dataloader)\n",
    "    while True:\n",
    "        images, labels = next(dataloader)\n",
    "        yield Batch(inputs=images, labels=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "model = 'inceptionv4'\n",
    "root_dir = '..'\n",
    "data_root_dir = os.path.join(root_dir, 'data')\n",
    "model_root_dir = os.path.join(root_dir, 'models')\n",
    "stages = ['test']\n",
    "\n",
    "\n",
    "# Paths setup\n",
    "data_dir = os.path.join(data_root_dir,'images')\n",
    "output_dir = os.path.join(model_root_dir,'classification/{}'.format(model))\n",
    "model_history = os.path.join(output_dir,'model_history.pkl')\n",
    "model_file = os.path.join(output_dir, 'caltech_birds_{}_full.pth'.format(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get image transforms\n",
    "# Get data transforms\n",
    "data_transforms = resizeCropTransforms(img_crop_size=224, img_resize=256)\n",
    "\n",
    "# Setup data loaders with augmentation transforms\n",
    "image_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x), data_transforms[x])\n",
    "                  for x in stages}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             shuffle=False, num_workers=2)\n",
    "              for x in stages}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in stages}\n",
    "class_names = image_datasets[stages[0]].classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device:: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Setup the device to run the computations\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print('Device::', device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.parallel.data_parallel.DataParallel' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.conv.Conv2d' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.activation.ReLU' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n",
      "/anaconda/envs/py37_pytorch/lib/python3.7/site-packages/torch/serialization.py:657: SourceChangeWarning: source code of class 'torch.nn.modules.linear.Linear' has changed. you can retrieve the original source code by accessing the object's source attribute or set `torch.nn.Module.dump_patches = True` and use the patch tool to revert the changes.\n",
      "  warnings.warn(msg, SourceChangeWarning)\n"
     ]
    }
   ],
   "source": [
    "# Load the best model from file\n",
    "model_ = torch.load(model_file)\n",
    "_ = model_.to(device).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualizer = AttributionVisualizer(\n",
    "    models=[model_],\n",
    "    score_func=lambda o: torch.nn.functional.softmax(o, 1),\n",
    "    classes=class_names,\n",
    "    features=[\n",
    "        ImageFeature(\n",
    "            \"Photo\",\n",
    "            baseline_transforms=[baseline_func],\n",
    "            input_transforms=[resizeCropTransforms()['test']],\n",
    "        )\n",
    "    ],\n",
    "    dataset=formatted_data_iter(dataloaders['test']),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fetch data and view Captum Insights at http://localhost:8600/\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8600"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "visualizer.serve(port=8600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Compose(\n",
       "     Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     ToTensor()\n",
       "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       " ), 'test': Compose(\n",
       "     Resize(size=256, interpolation=PIL.Image.BILINEAR)\n",
       "     CenterCrop(size=(224, 224))\n",
       "     ToTensor()\n",
       "     Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       " )}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resizeCropTransforms()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "py37_pytorch",
   "language": "python",
   "name": "conda-env-py37_pytorch-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
