{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets, models\n",
    "\n",
    "from imutils import paths\n",
    "from pathlib import Path\n",
    "import os\n",
    "import time\n",
    "import copy\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pylab as plt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "# Local modules\n",
    "from train import train_model\n",
    "from visualize import imshow, visualize_model\n",
    "from utils import unpickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Script runtime options\n",
    "models = ['googlenet','inception_v3','inceptionv4','pnasnet5large','resnet152','resnext101_32x8d','resnext101_64x4d']\n",
    "data_root_dir = 'data'\n",
    "model_root_dir = 'models'\n",
    "stages = ['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model training history\n",
    "history = {}\n",
    "class_report = {}\n",
    "confusion_matrix = {}\n",
    "for model in models:\n",
    "    data_dir = os.path.join(data_root_dir,'images')\n",
    "    output_dir = os.path.join(model_root_dir,'classification/{}'.format(model))\n",
    "    model_history = os.path.join(output_dir,'model_history.pkl')\n",
    "    model_file = os.path.join(output_dir, 'caltech_birds_{}_full.pth'.format(model))\n",
    "    \n",
    "    history[model] = unpickle(model_history)\n",
    "    class_report[model] = unpickle(os.path.join(output_dir,'classification_report.pkl'))\n",
    "    confusion_matrix[model] = unpickle(os.path.join(output_dir,'confusion_matrix.pkl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(18,10))\n",
    "\n",
    "plt.subplot(2,1,1)\n",
    "for model in models:\n",
    "    plt.plot(np.arange(0, np.max(history[model]['epoch'])+1,1), history[model]['test_loss'], label=model)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.yscale('log')\n",
    "plt.title('Validation Loss - Caltech Birds - Comparison of models')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(2,1,2)\n",
    "for model in models:\n",
    "    plt.plot(np.arange(0, np.max(history[model]['epoch'])+1,1), history[model]['test_acc'], label=model)\n",
    "plt.grid(True)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Validation Accuracy - Caltech Birds - Comparison of models')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "precision = []\n",
    "recall = []\n",
    "f1_score = []\n",
    "for model in models:\n",
    "    precision.append( class_report[model]['macro avg']['precision'] )\n",
    "    recall.append( class_report[model]['macro avg']['recall'] )\n",
    "    f1_score.append( class_report[model]['macro avg']['f1-score'] )\n",
    "    acc.append( class_report[model]['accuracy']['precision'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric_names = ['accuracy', 'precision', 'recall', 'f1_score']\n",
    "plt.figure(figsize=(10,3))\n",
    "fig, ax = plt.subplots()\n",
    "i_metric = 0\n",
    "axs = []\n",
    "for metric_name, metric in zip(metric_names, [acc, precision, recall, f1_score]):\n",
    "    axs.append(ax.bar(i_metric+0.35, models, metric, 0.35, label=metric_name))\n",
    "    plt.grid(True)\n",
    "    plt.ylim([0.5, 1.0])\n",
    "    plt.xlabel\n",
    "    #plt.title('Comparison of class average {}'.format(metric_name))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit ('py37_pytorch': conda)",
   "language": "python",
   "name": "python37664bitpy37pytorchconda5037f5dcc6574b5c975ef80bcc16f697"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6-final"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}